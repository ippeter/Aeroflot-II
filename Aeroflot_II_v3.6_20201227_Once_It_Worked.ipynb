{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use bonus plates (length of 4)\n",
    "## Important notes:\n",
    "### - new color model: number of channels = 4, one channel for each color  \n",
    "### - plate value encoded with a number: 0.1, 0.4, 1.0  \n",
    "### - v3.2: all helpers moved to external module\n",
    "### - v3.2: hint restored: s_before is now field after 'make_move'\n",
    "### - v3.3: debugging CNN since learning doesn't work\n",
    "### - v3.4: try bigger CNN \n",
    "### - v3.5: problem found. Large space of moves just doesn't work. Reduce to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# Needed for tests and a real game against the phone\n",
    "#import pandas as pd\n",
    "#import qgrid\n",
    "\n",
    "import random as rd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import environment as ae\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN\n",
    "LEARNING_RATE = 0.001\n",
    "UPDATE_TARGET_NET = 1000\n",
    "MINIBATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "\n",
    "# Definitions\n",
    "GAMES_TO_PLAY = 131072\n",
    "REPLAY_MEMORY_SIZE = 8192\n",
    "DYNAMIC_LEARNING_EPOCHS = 5\n",
    "NUMBER_OF_MOVES_IN_GAME = 50\n",
    "\n",
    "# Variables\n",
    "MAXIMUM_SCORE = 0\n",
    "CNN_MOVE_PROB = 0.1\n",
    "CNN_MOVES_COUNT = 0\n",
    "CNN_SUCCESSFUL_PREDICTION = 0\n",
    "AGG_GAMES_NUMBER = 50\n",
    "TOTAL_SCORE_AGG = 0.0\n",
    "TOTAL_SUCCESSFUL_MOVES_AGG = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Memory Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Replay memory buffer\n",
    "#\n",
    "class ExperienceBuffer():\n",
    "    '''\n",
    "    Experience Replay Buffer\n",
    "    Inspired by Andrea Lonza\n",
    "    '''\n",
    "\n",
    "    def __init__(self, buffer_size, gamma):\n",
    "        # Constants\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Main Replay Memory buffer parts\n",
    "        self.states_before = deque(maxlen=buffer_size)\n",
    "        self.actions = deque(maxlen=buffer_size)\n",
    "        self.total_rewards = deque(maxlen=buffer_size)\n",
    "        self.states_after = deque(maxlen=buffer_size)\n",
    "        self.last_moves = deque(maxlen=buffer_size)\n",
    "   \n",
    "    \n",
    "    def add(self, state_before, action, reward, state_after, last_move):\n",
    "        # Add certain items to corresponding buffers\n",
    "        self.states_before.append(state_before)\n",
    "        self.actions.append(action)\n",
    "        self.total_rewards.append(reward)\n",
    "        self.states_after.append(state_after)\n",
    "        self.last_moves.append(last_move)\n",
    "    \n",
    "    \n",
    "    def sample_minibatch(self, minibatch_size):\n",
    "        '''\n",
    "        Sample a minibatch of size batch_size\n",
    "        Note1: always add the most recent completed move\n",
    "        '''\n",
    "        indices = rd.sample(range(len(self.states_before) - 1), minibatch_size - 1)\n",
    "        # Add the most recent completed move index\n",
    "        indices.append(len(self.states_before) - 1)\n",
    "        \n",
    "        minibatch_states_before = np.array([self.states_before[i] for i in indices]) \n",
    "        minibatch_actions = np.array([self.actions[i] for i in indices]) \n",
    "        minibatch_total_rewards = np.array([self.total_rewards[i] for i in indices]) \n",
    "        minibatch_states_after = np.array([self.states_after[i] for i in indices])  \n",
    "        minibatch_last_moves = np.array([self.last_moves[i] for i in indices])   \n",
    "        \n",
    "        return minibatch_states_before, minibatch_actions, minibatch_total_rewards, minibatch_states_after, minibatch_last_moves\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Return length of the current replay memory buffer\n",
    "        Relevant for the first *minibatch_size* moves.\n",
    "        '''\n",
    "        return len(self.states_before)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Nework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_memory = ExperienceBuffer(REPLAY_MEMORY_SIZE, GAMMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Online CNN and Target CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(8)\n",
    "\n",
    "# Initialize optimizer\n",
    "#online_cnn_optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n",
    "online_cnn_optimizer = tf.keras.optimizers.SGD(LEARNING_RATE)\n",
    "\n",
    "# Online CNN\n",
    "Online_CNN = tf.keras.models.Sequential()\n",
    "Online_CNN.add(tf.keras.layers.Conv2D(128, kernel_size=3, strides = (1, 1), padding='same', activation=tf.nn.relu, data_format='channels_last', kernel_initializer='GlorotUniform', input_shape=(7, 6, 4)))\n",
    "Online_CNN.add(tf.keras.layers.BatchNormalization())\n",
    "Online_CNN.add(tf.keras.layers.Conv2D(128, kernel_size=3, strides = (1, 1), padding='same', activation=tf.nn.relu, data_format='channels_last', kernel_initializer='GlorotUniform', input_shape=(7, 6, 4)))\n",
    "Online_CNN.add(tf.keras.layers.BatchNormalization())\n",
    "Online_CNN.add(tf.keras.layers.Conv2D(64, kernel_size=3, strides = (1, 1), padding='same', activation=tf.nn.relu, kernel_initializer='GlorotUniform'))    \n",
    "Online_CNN.add(tf.keras.layers.BatchNormalization())\n",
    "Online_CNN.add(tf.keras.layers.Flatten())                      \n",
    "Online_CNN.add(tf.keras.layers.Dense(32, activation=tf.keras.activations.relu, kernel_initializer='GlorotUniform'))\n",
    "Online_CNN.add(tf.keras.layers.BatchNormalization())\n",
    "Online_CNN.add(tf.keras.layers.Dense(1, activation=tf.keras.activations.relu, kernel_initializer='GlorotUniform'))\n",
    "\n",
    "Online_CNN.compile(optimizer=online_cnn_optimizer, loss=\"mean_squared_error\")\n",
    "\n",
    "# Target CNN\n",
    "#Target_CNN = tf.keras.models.Sequential()\n",
    "#Target_CNN.add(tf.keras.layers.Conv2D(128, kernel_size=3, strides = (1, 1), padding='same', activation=tf.keras.activations.tanh, data_format = 'channels_last', kernel_initializer='RandomNormal', input_shape=(7, 6, 4)))\n",
    "#Target_CNN.add(tf.keras.layers.Conv2D(64, kernel_size=3, strides = (1, 1), padding='same', activation=tf.keras.activations.tanh, kernel_initializer='RandomNormal'))    \n",
    "#Target_CNN.add(tf.keras.layers.Flatten())                      \n",
    "#Target_CNN.add(tf.keras.layers.Dense(32, activation=tf.keras.activations.relu, kernel_initializer='RandomNormal'))\n",
    "#Target_CNN.add(tf.keras.layers.Dense(1, activation=tf.keras.activations.relu, kernel_initializer='RandomNormal'))\n",
    "\n",
    "#Target_CNN.compile(optimizer=online_cnn_optimizer, loss=\"mean_squared_error\")\n",
    "\n",
    "\n",
    "# Set weights equal\n",
    "#Target_CNN.set_weights(Online_CNN.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 7, 6, 128)         4736      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 7, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 7, 6, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 7, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 6, 64)          73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 7, 6, 64)          256       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                86048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 313,601\n",
      "Trainable params: 312,897\n",
      "Non-trainable params: 704\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Just to see how many trainable parameters\n",
    "Online_CNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of moves made to follow the target CNN update strategy\n",
    "total_moves = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def loss(model, X, y_true, A):\n",
    "#    prediction = model(X)\n",
    "#    selected_action_values = tf.math.reduce_sum(prediction*A, axis=1)  \n",
    "#    return tf.keras.losses.MSE(y_true, selected_action_values)\n",
    "\n",
    "\n",
    "#def grad(model, inputs, targets, actions):\n",
    "#    with tf.GradientTape() as tape:\n",
    "#        loss_value = loss(model, inputs, targets, actions)\n",
    "#    return loss_value, tape.gradient(loss_value, model.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New maximum: 37, after 0 games.\n",
      "New maximum: 68, after 2 games.\n",
      "New maximum: 154, after 4 games.\n",
      "Games: 50, last 50 games avg score: 59.7, avg of succ moves: 9.52, loss: [5.203125, 5.203125, 5.203125, 5.203125, 5.203125]\n",
      "CNN made 249 moves. Successful were 16\n",
      "rewards_next is 0.0\n",
      "Games: 100, last 50 games avg score: 54.98, avg of succ moves: 8.76, loss: [17.3125, 17.3125, 17.3125, 17.3125, 17.3125]\n",
      "CNN made 262 moves. Successful were 16\n",
      "rewards_next is 0.0\n",
      "Games: 150, last 50 games avg score: 57.44, avg of succ moves: 9.36, loss: [11.375, 11.375, 11.375, 11.375, 11.375]\n",
      "CNN made 263 moves. Successful were 14\n",
      "rewards_next is 0.0\n",
      "Games: 200, last 50 games avg score: 63.4, avg of succ moves: 9.42, loss: [21.5625, 21.5625, 21.5625, 21.5625, 21.5625]\n",
      "CNN made 248 moves. Successful were 18\n",
      "rewards_next is 0.0\n",
      "Games: 250, last 50 games avg score: 56.1, avg of succ moves: 9.02, loss: [22.625, 22.625, 22.625, 22.625, 22.625]\n",
      "CNN made 226 moves. Successful were 18\n",
      "rewards_next is 0.0\n",
      "Games: 300, last 50 games avg score: 61.66, avg of succ moves: 9.48, loss: [5.90625, 5.90625, 5.90625, 5.90625, 5.90625]\n",
      "CNN made 248 moves. Successful were 14\n",
      "rewards_next is 0.0\n",
      "Games: 350, last 50 games avg score: 60.88, avg of succ moves: 9.52, loss: [15.8125, 15.8125, 15.8125, 15.8125, 15.8125]\n",
      "CNN made 243 moves. Successful were 12\n",
      "rewards_next is 0.0\n",
      "New maximum: 158, after 363 games.\n",
      "Games: 400, last 50 games avg score: 68.08, avg of succ moves: 9.36, loss: [22.953125, 22.953125, 22.953125, 22.953125, 22.953125]\n",
      "CNN made 251 moves. Successful were 20\n",
      "rewards_next is 0.0\n",
      "Games: 450, last 50 games avg score: 63.6, avg of succ moves: 9.5, loss: [22.359375, 22.359375, 22.359375, 22.359375, 22.359375]\n",
      "CNN made 214 moves. Successful were 15\n",
      "rewards_next is 0.0\n",
      "Games: 500, last 50 games avg score: 53.86, avg of succ moves: 8.18, loss: [17.546875, 17.546875, 17.546875, 17.546875, 17.546875]\n",
      "CNN made 260 moves. Successful were 12\n",
      "rewards_next is 0.0\n",
      "Games: 550, last 50 games avg score: 59.36, avg of succ moves: 9.74, loss: [2.359375, 2.359375, 2.359375, 2.359375, 2.359375]\n",
      "CNN made 217 moves. Successful were 24\n",
      "rewards_next is 0.0\n",
      "Games: 600, last 50 games avg score: 50.96, avg of succ moves: 8.74, loss: [8.1875, 8.1875, 8.1875, 8.1875, 8.1875]\n",
      "CNN made 502 moves. Successful were 24\n",
      "rewards_next is 0.0\n",
      "Games: 650, last 50 games avg score: 54.7, avg of succ moves: 8.16, loss: [1.890625, 1.890625, 1.890625, 1.890625, 1.890625]\n",
      "CNN made 513 moves. Successful were 22\n",
      "rewards_next is 0.0\n",
      "Games: 700, last 50 games avg score: 62.66, avg of succ moves: 9.34, loss: [8.125, 8.125, 8.125, 8.125, 8.125]\n",
      "CNN made 502 moves. Successful were 16\n",
      "rewards_next is 0.0\n",
      "Games: 750, last 50 games avg score: 52.06, avg of succ moves: 7.8, loss: [16.09375, 16.09375, 16.09375, 16.09375, 16.09375]\n",
      "CNN made 520 moves. Successful were 18\n",
      "rewards_next is 0.0\n",
      "Games: 800, last 50 games avg score: 56.94, avg of succ moves: 8.48, loss: [10.28125, 10.28125, 10.28125, 10.28125, 10.28125]\n",
      "CNN made 514 moves. Successful were 22\n",
      "rewards_next is 0.0\n",
      "Games: 850, last 50 games avg score: 52.0, avg of succ moves: 8.14, loss: [7.84375, 7.84375, 7.84375, 7.84375, 7.84375]\n",
      "CNN made 501 moves. Successful were 17\n",
      "rewards_next is 0.0\n",
      "Games: 900, last 50 games avg score: 49.62, avg of succ moves: 7.66, loss: [17.5, 17.5, 17.5, 17.5, 17.5]\n",
      "CNN made 493 moves. Successful were 17\n",
      "rewards_next is 0.0\n",
      "Games: 950, last 50 games avg score: 57.4, avg of succ moves: 8.2, loss: [27.640625, 27.640625, 27.640625, 27.640625, 27.640625]\n",
      "CNN made 504 moves. Successful were 34\n",
      "rewards_next is 0.0\n",
      "Games: 1000, last 50 games avg score: 57.84, avg of succ moves: 8.22, loss: [21.953125, 21.953125, 21.953125, 21.953125, 21.953125]\n",
      "CNN made 525 moves. Successful were 20\n",
      "rewards_next is 0.0\n",
      "Games: 1050, last 50 games avg score: 56.8, avg of succ moves: 8.42, loss: [13.140625, 13.140625, 13.140625, 13.140625, 13.140625]\n",
      "CNN made 512 moves. Successful were 20\n",
      "rewards_next is 0.0\n",
      "Games: 1100, last 50 games avg score: 51.78, avg of succ moves: 8.56, loss: [12.171875, 12.171875, 12.171875, 12.171875, 12.171875]\n",
      "CNN made 492 moves. Successful were 21\n",
      "rewards_next is 0.0\n",
      "Games: 1150, last 50 games avg score: 55.96, avg of succ moves: 8.72, loss: [4.765625, 4.765625, 4.765625, 4.765625, 4.765625]\n",
      "CNN made 466 moves. Successful were 16\n",
      "rewards_next is 0.0\n",
      "Games: 1200, last 50 games avg score: 56.28, avg of succ moves: 8.2, loss: [11.34375, 11.34375, 11.34375, 11.34375, 11.34375]\n",
      "CNN made 468 moves. Successful were 17\n",
      "rewards_next is 0.0\n",
      "Games: 1250, last 50 games avg score: 47.76, avg of succ moves: 7.86, loss: [11.859375, 11.859375, 11.859375, 11.859375, 11.859375]\n",
      "CNN made 494 moves. Successful were 18\n",
      "rewards_next is 0.0\n",
      "Games: 1300, last 50 games avg score: 57.72, avg of succ moves: 8.46, loss: [5.3125, 5.3125, 5.3125, 5.3125, 5.3125]\n",
      "CNN made 504 moves. Successful were 12\n",
      "rewards_next is 0.0\n",
      "Games: 1350, last 50 games avg score: 55.04, avg of succ moves: 8.34, loss: [41.96875, 41.96875, 41.96875, 41.96875, 41.96875]\n",
      "CNN made 484 moves. Successful were 20\n",
      "rewards_next is 0.0\n",
      "Games: 1400, last 50 games avg score: 49.82, avg of succ moves: 8.3, loss: [13.046875, 13.046875, 13.046875, 13.046875, 13.046875]\n",
      "CNN made 518 moves. Successful were 27\n",
      "rewards_next is 0.0\n",
      "Games: 1450, last 50 games avg score: 54.48, avg of succ moves: 7.6, loss: [19.171875, 19.171875, 19.171875, 19.171875, 19.171875]\n",
      "CNN made 515 moves. Successful were 18\n",
      "rewards_next is 0.0\n",
      "Games: 1500, last 50 games avg score: 53.62, avg of succ moves: 7.86, loss: [11.796875, 11.796875, 11.796875, 11.796875, 11.796875]\n",
      "CNN made 535 moves. Successful were 22\n",
      "rewards_next is 0.0\n",
      "Games: 1550, last 50 games avg score: 51.26, avg of succ moves: 8.02, loss: [3.953125, 3.953125, 3.953125, 3.953125, 3.953125]\n",
      "CNN made 519 moves. Successful were 17\n",
      "rewards_next is 0.0\n",
      "Games: 1600, last 50 games avg score: 53.44, avg of succ moves: 8.5, loss: [7.453125, 7.453125, 7.453125, 7.453125, 7.453125]\n",
      "CNN made 505 moves. Successful were 24\n",
      "rewards_next is 0.0\n",
      "Games: 1650, last 50 games avg score: 51.96, avg of succ moves: 8.0, loss: [5.515625, 5.515625, 5.515625, 5.515625, 5.515625]\n",
      "CNN made 536 moves. Successful were 17\n",
      "rewards_next is 0.0\n",
      "Games: 1700, last 50 games avg score: 52.54, avg of succ moves: 8.1, loss: [24.703125, 24.703125, 24.703125, 24.703125, 24.703125]\n",
      "CNN made 501 moves. Successful were 22\n",
      "rewards_next is 0.0\n",
      "Games: 1750, last 50 games avg score: 54.64, avg of succ moves: 8.4, loss: [5.6875, 5.6875, 5.6875, 5.6875, 5.6875]\n",
      "CNN made 531 moves. Successful were 20\n",
      "rewards_next is 0.0\n",
      "Games: 1800, last 50 games avg score: 51.2, avg of succ moves: 8.1, loss: [43.265625, 43.265625, 43.265625, 43.265625, 43.265625]\n",
      "CNN made 512 moves. Successful were 16\n",
      "rewards_next is 0.0\n",
      "Games: 1850, last 50 games avg score: 54.6, avg of succ moves: 8.48, loss: [3.203125, 3.203125, 3.203125, 3.203125, 3.203125]\n",
      "CNN made 501 moves. Successful were 24\n",
      "rewards_next is 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \"\"\"\n\u001b[0;32m-> 1843\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "FOUND = False\n",
    "                                                      \n",
    "for game in range(GAMES_TO_PLAY):\n",
    "    # Start one game\n",
    "    game_score = 0\n",
    "    successful_moves = 0\n",
    "\n",
    "    # Initialize the game field\n",
    "    field = np.zeros((7, 6, 4))\n",
    "    field = ae.initialize_field_3D(field)\n",
    "\n",
    "    for m in range(NUMBER_OF_MOVES_IN_GAME):\n",
    "        # Total score of one move\n",
    "        reward = 0\n",
    "\n",
    "        # Whether CNN made the move\n",
    "        cnn_made_move_flag = False\n",
    "        \n",
    "        # If replay_memory has less than 64 moves, then make a random move\n",
    "        if ((len(replay_memory) < MINIBATCH_SIZE) or (rd.random() > CNN_MOVE_PROB)):\n",
    "            move = rd.randint(1, ae.ACTIONS_DIMENSION)\n",
    "        else:\n",
    "            # CNN selects a move\n",
    "            cnn_made_move_flag = True\n",
    "            CNN_MOVES_COUNT = CNN_MOVES_COUNT + 1\n",
    "            #_, move = ae.predict_max_score_3D(field.copy(), Target_CNN, ae.ACTIONS_DIMENSION, ae.MOVES)\n",
    "            _, move = ae.predict_max_score_3D(field.copy(), Online_CNN, ae.ACTIONS_DIMENSION, ae.MOVES)\n",
    "\n",
    "            \n",
    "        # Make the move\n",
    "        new_field, plate_a, plate_b = ae.make_move_v2_3D(field.copy(), move, ae.MOVES)\n",
    "        field_after_move = new_field.copy()\n",
    "\n",
    "        # Calculate the score and update the field\n",
    "        score, new_field = ae.calculate_score_v2_3D(new_field, plate_a, plate_b)\n",
    "        \n",
    "        # If the move is successful, then update the field and check if we have new sets        \n",
    "        successful_move_flag = False\n",
    "\n",
    "        # While we have new sets (thus the score is greater than 0), process them, calculate score and move plates\n",
    "        while (score > 0.):\n",
    "            if (not successful_move_flag):\n",
    "                successful_moves = successful_moves + 1\n",
    "                successful_move_flag = True\n",
    "\n",
    "            # Add new points to the total score of the move\n",
    "            reward = reward + score\n",
    "\n",
    "            # Move plates downward, fill the upper row so, that it doesn't have \"easy\" sets of three\n",
    "            # Start from the left lower corner (in order to reuse color_fits())\n",
    "            new_field = ae.fill_field_3D(new_field)\n",
    "\n",
    "            # Calculate score and check whether we have new sets\n",
    "            score, new_field = ae.calculate_score_v2_3D(new_field, (-1, -1), (-1, -1))\n",
    "\n",
    "        # Increase the score of the whole game\n",
    "        game_score = game_score + reward\n",
    "        \n",
    "        # Check whether it's the last move of the current game\n",
    "        last_move = m == NUMBER_OF_MOVES_IN_GAME - 1         \n",
    "\n",
    "        # Add new move to the replay memory\n",
    "        if (successful_move_flag):\n",
    "            replay_memory.add(field_after_move, move, reward, new_field, last_move)\n",
    "            \n",
    "            # Update CNN move statistics\n",
    "            if (cnn_made_move_flag):    \n",
    "                CNN_SUCCESSFUL_PREDICTION = CNN_SUCCESSFUL_PREDICTION + 1               \n",
    "                \n",
    "        else:\n",
    "            replay_memory.add(field_after_move, move, 0, field, last_move)   \n",
    "        \n",
    "        #\n",
    "        # Train CNN based on the score\n",
    "        #\n",
    "        if (len(replay_memory) >= MINIBATCH_SIZE):\n",
    "            # Select random MINIBATCH_SIZE moves from replay memory buffer\n",
    "            samples = replay_memory.sample_minibatch(MINIBATCH_SIZE)\n",
    "\n",
    "            # Prepare some things for training\n",
    "            s_before = samples[0]\n",
    "            actions = samples[1]\n",
    "            rewards = samples[2]\n",
    "            s_after = samples[3] \n",
    "            dones = samples[4]\n",
    "            \n",
    "            # Carefully predict next rewards: we must use predict_max_score\n",
    "            rewards_next = np.zeros((64))\n",
    "            \n",
    "            for item in range(MINIBATCH_SIZE):\n",
    "                # Calculate future rewards ONLY if current reward is NOT zero, otherwise no sense\n",
    "                if (rewards[item] > 0):\n",
    "                    #rewards_next[item], _ = ae.predict_max_score_3D(s_after[item], Target_CNN, ae.ACTIONS_DIMENSION, ae.MOVES)\n",
    "                    rewards_next[item], _ = ae.predict_max_score_3D(s_after[item], Online_CNN, ae.ACTIONS_DIMENSION, ae.MOVES)\n",
    "\n",
    "\n",
    "            #\n",
    "            # Update online CNN weights: training step\n",
    "            #\n",
    "            actual_values = np.where(dones, rewards, rewards + GAMMA*rewards_next)\n",
    "            \n",
    "            #\n",
    "            # This one doesn't work\n",
    "            #\n",
    "            \n",
    "            #selected_actions = tf.one_hot(actions - 1, ae.ACTIONS_DIMENSION)\n",
    "            \n",
    "            #for _ in range(DYNAMIC_LEARNING_EPOCHS):             \n",
    "            #    loss_value, grads = grad(Online_CNN, s_before, actual_values, selected_actions)\n",
    "            #    optimizer.apply_gradients(zip(grads, Online_CNN.trainable_variables))\n",
    "            \n",
    "            history = Online_CNN.fit(x=s_before, y=actual_values, epochs=DYNAMIC_LEARNING_EPOCHS, verbose=0)\n",
    "            \n",
    "            # DEBUG: Trying to find a reason of NaN\n",
    "            for layer in Online_CNN.layers:\n",
    "                weights = layer.get_weights()\n",
    "                for w in weights:\n",
    "                    if (np.isnan(w).sum() > 0):\n",
    "                        FOUND = True\n",
    "                        break\n",
    "                if (FOUND):\n",
    "                    break\n",
    "                    \n",
    "        if (FOUND):\n",
    "            print(\"NaN value found!\")\n",
    "            break\n",
    "\n",
    "        # If move is successful, update the play field\n",
    "        if (successful_move_flag):\n",
    "            field = np.copy(new_field)\n",
    "            \n",
    "        # After each UPDATE_TARGET_NET moves update target CNN\n",
    "        #if (total_moves % UPDATE_TARGET_NET == 0):\n",
    "        #    Target_CNN.set_weights(Online_CNN.get_weights())\n",
    "            \n",
    "        total_moves = total_moves + 1\n",
    "\n",
    "    if (FOUND):\n",
    "        break\n",
    "        \n",
    "    #\n",
    "    # Calculate and display overall stats\n",
    "    #\n",
    "    # Check whether we have new maximum score\n",
    "    if (game_score > MAXIMUM_SCORE):\n",
    "        print(f\"New maximum: {game_score}, after {game} games.\")\n",
    "        MAXIMUM_SCORE = game_score\n",
    "        \n",
    "    # After each AGG_GAMES_NUMBER games output average game score, average number of successful moves per game\n",
    "    TOTAL_SCORE_AGG = TOTAL_SCORE_AGG + game_score\n",
    "    TOTAL_SUCCESSFUL_MOVES_AGG = TOTAL_SUCCESSFUL_MOVES_AGG + successful_moves\n",
    "    \n",
    "    if ((game % AGG_GAMES_NUMBER == 0) and (game > 0)):\n",
    "        avg_score = TOTAL_SCORE_AGG / AGG_GAMES_NUMBER\n",
    "        TOTAL_SCORE_AGG = 0.0\n",
    "        \n",
    "        avg_succ_moves = TOTAL_SUCCESSFUL_MOVES_AGG / AGG_GAMES_NUMBER\n",
    "        TOTAL_SUCCESSFUL_MOVES_AGG = 0.0\n",
    "\n",
    "        loss_value = history.history[\"loss\"]\n",
    "        print(f\"Games: {game}, last {AGG_GAMES_NUMBER} games avg score: {avg_score}, avg of succ moves: {avg_succ_moves}, loss: {loss_value}\")        \n",
    "        print(f\"CNN made {CNN_MOVES_COUNT} moves. Successful were {CNN_SUCCESSFUL_PREDICTION}\")\n",
    "        print(f\"rewards_next is {rewards_next.sum()}\")\n",
    "        \n",
    "        if (CNN_SUCCESSFUL_PREDICTION / CNN_MOVES_COUNT >= CNN_MOVE_PROB):\n",
    "            CNN_MOVE_PROB = CNN_MOVE_PROB + 0.1\n",
    "            \n",
    "        CNN_MOVES_COUNT = 0\n",
    "        CNN_SUCCESSFUL_PREDICTION = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Online_CNN.fit(x=s_before, y=actual_values, epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_next.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(dones, rewards, rewards + GAMMA*rewards_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Online_CNN.predict(np.expand_dims(s_before[1], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(s_before).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for layer in Online_CNN.layers:\n",
    "    weights = layer.get_weights()\n",
    "    print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(weights[0]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debuggin the Hint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total score of one move\n",
    "reward = 0\n",
    "\n",
    "# Whether CNN made the move\n",
    "cnn_made_move_flag = False\n",
    "\n",
    "# If replay_memory has less than 64 moves, then make a random move\n",
    "if ((len(replay_memory) < MINIBATCH_SIZE) or (rd.random() > CNN_MOVE_PROB)):\n",
    "    move = rd.randint(1, ae.ACTIONS_DIMENSION)\n",
    "else:\n",
    "    # CNN selects a move\n",
    "    cnn_made_move_flag = True\n",
    "    CNN_MOVES_COUNT = CNN_MOVES_COUNT + 1\n",
    "    scr, move = ae.predict_max_score_3D(field.copy(), Target_CNN, ae.ACTIONS_DIMENSION, ae.MOVES)\n",
    "    print(f\"Predicted move {move} with score of {scr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the move\n",
    "new_field, plate_a, plate_b = ae.make_move_v2_3D(field.copy(), move, ae.MOVES)\n",
    "field_after_move = new_field.copy()\n",
    "\n",
    "# Calculate the score and update the field\n",
    "score, new_field = ae.calculate_score_v2_3D(new_field, plate_a, plate_b)\n",
    "\n",
    "# If the move is successful, then update the field and check if we have new sets        \n",
    "successful_move_flag = False\n",
    "\n",
    "# While we have new sets (thus the score is greater than 0), process them, calculate score and move plates\n",
    "while (score > 0.):\n",
    "    if (not successful_move_flag):\n",
    "        successful_moves = successful_moves + 1\n",
    "        successful_move_flag = True\n",
    "\n",
    "    # Add new points to the total score of the move\n",
    "    reward = reward + score\n",
    "\n",
    "    # Move plates downward, fill the upper row so, that it doesn't have \"easy\" sets of three\n",
    "    # Start from the left lower corner (in order to reuse color_fits())\n",
    "    new_field = ae.fill_field_3D(new_field)\n",
    "\n",
    "    # Calculate score and check whether we have new sets\n",
    "    score, new_field = ae.calculate_score_v2_3D(new_field, (-1, -1), (-1, -1))\n",
    "\n",
    "print(reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "# Train CNN based on the score\n",
    "#\n",
    "\n",
    "samples = replay_memory.sample_minibatch(MINIBATCH_SIZE)\n",
    "\n",
    "# Prepare some things for training\n",
    "s_before = samples[0]\n",
    "actions = samples[1]\n",
    "rewards = samples[2]\n",
    "s_after = samples[3] \n",
    "dones = samples[4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(model, X, y_true, A):\n",
    "    prediction = model(X)\n",
    "    selected_action_values = tf.math.reduce_sum(prediction*A, axis=1)  \n",
    "    return tf.keras.losses.MSE(y_true, selected_action_values)\n",
    "\n",
    "\n",
    "def grad(model, inputs, targets, actions):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets, actions)\n",
    "    return loss_value, tape.gradient(loss_value, model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_next = np.max(Target_CNN(s_after), axis=1)\n",
    "actual_values = np.where(dones, rewards, rewards + GAMMA*rewards_next)\n",
    "selected_actions = tf.one_hot(actions, ae.ACTIONS_DIMENSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_value, grads = grad(Online_CNN, s_before, actual_values, selected_actions)\n",
    "\n",
    "print(\"Step: {}, Initial Loss: {}\".format(optimizer.iterations.numpy(), loss_value.numpy()))\n",
    "\n",
    "optimizer.apply_gradients(zip(grads, Online_CNN.trainable_variables))\n",
    "\n",
    "print(\"Step: {}, Loss: {}\".format(optimizer.iterations.numpy(), loss(Online_CNN, s_before, actual_values, selected_actions).numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now the CNN has been trained.\n",
    "### Start the long reinforcement-learning cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "successful_moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_field = make_move(field, move)\n",
    "print(new_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_field = calculate_score(new_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_field = np.multiply(new_field, 1.0 - temp_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_field(new_field, colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Save model\n",
    "#\n",
    "# v1: 20190329, trained on len(replay_memory) = 294912\n",
    "#aero_cnn.save(\"Aero_CNN_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Create the moves dictionary\n",
    "#\n",
    "moves = {}\n",
    "\n",
    "for i in range(1, 143):\n",
    "    old_row, old_column, old_direction = process_move_142(i)\n",
    "    \n",
    "    start_row = old_row - 1\n",
    "    start_col = old_column - 1\n",
    "    \n",
    "    if (old_direction == \"down\"):\n",
    "        end_row = start_row + 1\n",
    "        end_col = start_col\n",
    "    elif (old_direction == \"up\"):\n",
    "        end_row = start_row - 1\n",
    "        end_col = start_col\n",
    "    elif (old_direction == \"right\"):\n",
    "        end_row = start_row\n",
    "        end_col = start_col + 1\n",
    "    else:\n",
    "        end_row = start_row\n",
    "        end_col = start_col - 1\n",
    "        \n",
    "    moves[i] = ((start_row, start_col), (end_row, end_col))\n",
    "    \n",
    "print(moves)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
